{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbed436",
   "metadata": {
    "id": "3fbed436"
   },
   "source": [
    "- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)\n",
    "- [NIPS 2016 Tutorial:\n",
    "Generative Adversarial Networks](https://arxiv.org/pdf/1701.00160.pdf)\n",
    "- [image source](https://xiaohongliu.ca/post/gan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeafda3",
   "metadata": {
    "id": "8eeafda3"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146280a7",
   "metadata": {
    "id": "146280a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d08cc45",
   "metadata": {
    "id": "3d08cc45"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21bcf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a7b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f13e0f",
   "metadata": {
    "id": "36f13e0f"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9dc5324",
   "metadata": {
    "id": "d9dc5324"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "학습에 사용될 hyperparameter 값들을 넣을 class를 정의합니다.\n",
    "\"\"\"\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb580a80",
   "metadata": {
    "id": "bb580a80"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN model 학습에 사용되는 결과 이미지 저장 경로, 에포크 수, 모델 입력 이미지 크기 등을 정의합니다.\n",
    "\"\"\"\n",
    "config = AttrDict()\n",
    "config.data_path = 'data/resource/' \n",
    "config.save_path = 'save/basic/'\n",
    "config.dataset = 'emoji' #CIFAR10 어떠한 데이터셋을 사용하는지\n",
    "config.n_epoch = 500\n",
    "config.log_interval = 88 # loss 출력\n",
    "config.save_interval = 20  # 이미지 출력\n",
    "config.batch_size = 100\n",
    "config.learning_rate = 0.001\n",
    "config.b1 = 0.5\n",
    "config.b2 = 0.999\n",
    "config.img_shape = (3, 128, 128) # c, w, h\n",
    "config.latent_size = 100 # random noise size\n",
    "\"\"\"\n",
    "모델 입력 이미지에 수행할 normalization과 모델 생성 결과 이미지에 수행할 denormalization을 정의합니다.\n",
    "\"\"\"\n",
    "config.augmentation = transforms.Compose([\n",
    "                        transforms.Resize((config.img_shape[1], config.img_shape[2])), #resize\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.5], std=[0.5]) #normalization\n",
    "                      ])\n",
    "config.denormalize = lambda x: x*0.5+0.5 #denormalization 위의 단계 reverse , 원래의 이미지를 보기 위해 수행함\n",
    "\n",
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #cuda에 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b950712e",
   "metadata": {
    "id": "b950712e"
   },
   "outputs": [],
   "source": [
    "#지정된 경로의 데이터셋 가져오기 및 저장\n",
    "if not os.path.isdir(config.data_path):\n",
    "    os.makedirs(config.data_path)\n",
    "if not os.path.isdir(os.path.join(config.save_path, config.dataset)):\n",
    "    os.makedirs(os.path.join(config.save_path, config.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29018cfd",
   "metadata": {
    "id": "29018cfd",
    "outputId": "38d86d46-3dc3-4e0f-ad44-7efd5d9a3da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc2c6a",
   "metadata": {
    "id": "3fbc2c6a"
   },
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12be5c3b",
   "metadata": {
    "id": "12be5c3b",
    "outputId": "233c623f-3f2e-4b90-df50-14f3ea03ab33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 8717\n",
      "    Root location: data/resource/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST와 CIFAR-10은 torchvision 라이브러리에서 제공하여 아래와 같이 사용할 수 있습니다.\n",
    "\"\"\"\n",
    "if config.dataset == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(config.data_path,\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=config.augmentation\n",
    "                                  ) \n",
    "elif config.dataset == 'CIFAR10': \n",
    "    train_dataset = datasets.CIFAR10(config.data_path,\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=config.augmentation\n",
    "                                     )\n",
    "elif config.dataset == 'emoji': \n",
    "    train_dataset = datasets.ImageFolder(config.data_path,\n",
    "                                       transform=config.augmentation\n",
    "                                     )\n",
    "\n",
    "print(config.dataset)\n",
    "    \n",
    "\"\"\"\n",
    "training set을 Dataloader에 넣습니다. \n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e22dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8717"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff42e1c",
   "metadata": {},
   "source": [
    "## custom data\n",
    "\n",
    "로컬 디렉토리 image 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ffa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688ba052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = ''\n",
    "glob.glob('data/resource/emoji/*')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a936d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x21d142f7280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba67a4c",
   "metadata": {
    "id": "6ba67a4c"
   },
   "source": [
    "## GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3608b5",
   "metadata": {
    "id": "7f3608b5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 일반적으로, GAN에서는 loss가 Discriminator에서부터 Generator로 흐를 때 생길 수 있는 \n",
    " vanishing gradient 현상을 완화하기 위해 Leaky ReLU를 많이 사용합니다. \n",
    "\"\"\"\n",
    "class Generator(nn.Module): # 입력으로 noise를 받음\n",
    "    def __init__(self, config):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self.block(config.latent_size, 128, batchnorm=False),\n",
    "            *self.block(128, 256),\n",
    "            *self.block(256, 512),\n",
    "            *self.block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(config.img_shape))), # 최종 결과는 이미지 형태로 나와야함\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.reshape(img.shape[0], *config.img_shape)\n",
    "        return img\n",
    "    \n",
    "    # 반복될 때 쓰는 함수 (더 확인해보자)\n",
    "    def block(self, input_size, output_size, batchnorm=True):\n",
    "        layers = [nn.Linear(input_size, output_size)]\n",
    "        if batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(output_size))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c10c2717",
   "metadata": {
    "id": "c10c2717"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(config.img_shape)), 512), # Linear의 경우 reshape 해서 써야함\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        validity = self.model(img)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9dc4bf",
   "metadata": {
    "id": "3b9dc4bf"
   },
   "source": [
    "#### Binary Cross Entropy loss between the target and the input probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684f084",
   "metadata": {
    "id": "9684f084"
   },
   "source": [
    "- [torch.nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9eac2a",
   "metadata": {
    "id": "fd9eac2a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "binary cross entropy loss를 사용하여 adversarial loss를 구현합니다.\n",
    "\"\"\"\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\"\"\"\n",
    "Generator와 Discriminator를 각각 정의하고, 상응하는 optimizer도 함께 정의합니다.\n",
    "\"\"\"\n",
    "generator = Generator(config).to(config.device)\n",
    "discriminator = Discriminator(config).to(config.device)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9f5670",
   "metadata": {
    "id": "3c9f5670",
    "outputId": "f57c1e89-69d8-46b5-f8b6-cb86430f25b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (11): Linear(in_features=1024, out_features=49152, bias=True)\n",
       "  (12): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4ca9ee",
   "metadata": {
    "id": "cf4ca9ee",
    "outputId": "95a7777c-fc6d-4c55-cf29-7aa6fc6ed9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=49152, out_features=512, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200b0b3",
   "metadata": {
    "id": "b200b0b3"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "482c07e6",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ef5466833de7413dac10e9bce3d17184"
     ]
    },
    "id": "482c07e6",
    "outputId": "61603b83-f92e-4e40-c146-eabf74bb5b65",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a029ed26b3480e89932929df08cd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/500] Batch [88/88] Discriminator loss: 50.0000 Generator loss: 0.0000\n",
      "\n",
      "Epoch [2/500] Batch [88/88] Discriminator loss: 50.0000 Generator loss: 0.0000\n",
      "\n",
      "Epoch [3/500] Batch [88/88] Discriminator loss: 50.0000 Generator loss: 0.0000\n",
      "\n",
      "Epoch [4/500] Batch [88/88] Discriminator loss: 50.0000 Generator loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#if epoch % 10 == 0:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#print(epoch,\"-------\")\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (real_img, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     15\u001b[0m     real_img \u001b[38;5;241m=\u001b[39m real_img\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    adversarial loss에 사용될 ground truth들입니다.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Discriminator에게 있어 실제 이미지는 1, generator가 생성한 fake 이미지는 0을 label로 합니다.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    반대로 Generator는 자신이 생성한 fake 이미지의 label이 1이 되게 하여 Discriminator를 fooling 합니다.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\"\"\"\n",
    "Generator와 Discriminator를 번갈아 학습합니다.\n",
    "\"\"\"\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "for epoch in tqdm(range(config.n_epoch)):\n",
    "    print()\n",
    "    #if epoch % 10 == 0:\n",
    "        #print(epoch,\"-------\")\n",
    "    for i, (real_img, _) in enumerate(train_loader):\n",
    "        real_img = real_img.to(config.device)\n",
    "\n",
    "        \"\"\"\n",
    "        adversarial loss에 사용될 ground truth들입니다.\n",
    "        Discriminator에게 있어 실제 이미지는 1, generator가 생성한 fake 이미지는 0을 label로 합니다.\n",
    "        반대로 Generator는 자신이 생성한 fake 이미지의 label이 1이 되게 하여 Discriminator를 fooling 합니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        valid_label = torch.ones((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        fake_label = torch.zeros((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                Train Discriminator                  #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        # 가짜 이미지 생성\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "        gen_img = generator(z)\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Discriminator가 실제 이미지와 Generator가 생성한 이미지를 잘 구별하는지 loss를 계산합니다.\n",
    "        이 때, Generator는 현재 계산된 loss로 학습되지 않으므로, \n",
    "        detach() 함수를 이용하여 생성 이미지를 computation graph에서 분리한 후 Discriminator의 입력으로 넣어줍니다. \n",
    "        \"\"\"\n",
    "        \n",
    "        ## loss 구함\n",
    "        real_loss = criterion(discriminator(real_img), valid_label)\n",
    "        fake_loss = criterion(discriminator(gen_img.detach()), fake_label) # detach generator는 가중치 업데이트 안함\n",
    "        d_loss = (real_loss + fake_loss) * 0.5\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Discriminator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward() # 여기서 grediant가 업데이트 안됨\n",
    "        optimizer_d.step()\n",
    "\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                   Train Generator                   #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "        gen_img = generator(z)\n",
    "\n",
    "        \"\"\"\n",
    "        Generator가 Discriminator를 속일 수 있는지 loss를 계산합니다.\n",
    "        \"\"\"\n",
    "        g_loss = criterion(discriminator(gen_img), valid_label) #valid_label로 하여 generator를 학습하게 함\n",
    "        \n",
    " \n",
    "        \"\"\"\n",
    "        Generator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if (i+1) % config.log_interval == 0:\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            d_loss_list.append(d_loss.item())\n",
    "            print('Epoch [{}/{}] Batch [{}/{}] Discriminator loss: {:.4f} Generator loss: {:.4f}'.format(\n",
    "                epoch+1,\n",
    "                config.n_epoch,\n",
    "                i+1,\n",
    "                len(train_loader),\n",
    "                d_loss.item(),\n",
    "                g_loss.item()\n",
    "            ))\n",
    "            \n",
    "    if (epoch+1) % config.save_interval == 0:\n",
    "        print('저장')\n",
    "        save_path = os.path.join(config.save_path, config.dataset, 'epoch_[{}].png'.format(epoch+1))\n",
    "        gen_img = config.denormalize(gen_img)\n",
    "        torchvision.utils.save_image(gen_img.data[:16], save_path, nrow=4, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0658ce",
   "metadata": {
    "id": "0e0658ce",
    "outputId": "f697deea-a400-44f8-8133-0bc0b179596c"
   },
   "outputs": [],
   "source": [
    "plt.title('GAN training loss on {} data'.format(config.dataset))\n",
    "plt.plot(g_loss_list, label='generator loss')\n",
    "plt.plot(d_loss_list, label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18473b",
   "metadata": {
    "id": "3b18473b"
   },
   "source": [
    "## Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b0509",
   "metadata": {
    "id": "db1b0509",
    "outputId": "71f5ad53-0e15-4dd8-ac74-29a7126017ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(config.save_path, config.dataset)\n",
    "for image_path in os.listdir(save_path):\n",
    "    if image_path.endswith('.png'):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        image = Image.open(os.path.join(save_path, image_path))\n",
    "        plt.title(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee394e6",
   "metadata": {
    "id": "1ee394e6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "basis_3.8",
   "language": "python",
   "name": "basis_3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e361de83e42006260aed2b055f8f04aac5dcd7e3eec4c7f8d8ccd6db537cc702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
