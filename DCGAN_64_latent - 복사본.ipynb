{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eeafda3",
   "metadata": {
    "id": "8eeafda3"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146280a7",
   "metadata": {
    "id": "146280a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d08cc45",
   "metadata": {
    "id": "3d08cc45"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21bcf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a7b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5657cabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2abc1cc0830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f13e0f",
   "metadata": {
    "id": "36f13e0f"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9dc5324",
   "metadata": {
    "id": "d9dc5324"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "학습에 사용될 hyperparameter 값들을 넣을 class를 정의합니다.\n",
    "\"\"\"\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb580a80",
   "metadata": {
    "id": "bb580a80"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN model 학습에 사용되는 결과 이미지 저장 경로, 에포크 수, 모델 입력 이미지 크기 등을 정의합니다.\n",
    "\"\"\"\n",
    "config = AttrDict()\n",
    "config.data_path = 'data/resource/' \n",
    "config.save_path = 'save/DCGAN_64_latent/'\n",
    "config.dataset = 'emoji' #CIFAR10 어떠한 데이터셋을 사용하는지\n",
    "#config.dataset = 'CIFAR10' #CIFAR10 어떠한 데이터셋을 사용하는지\n",
    "\n",
    "config.n_epoch = 200\n",
    "config.log_interval = 59 # loss 출력\n",
    "config.save_interval = 10  # 이미지 출력\n",
    "config.batch_size = 150\n",
    "\n",
    "config.nz = 100\n",
    "config.ngf = 128 # generator oonv filter\n",
    "config.ndf = 128 # discriminator oonv fillter\n",
    "\n",
    "config.learning_rate = 0.001\n",
    "config.b1 = 0.5\n",
    "config.b2 = 0.999\n",
    "config.img_shape = (3, 64, 64) # c, w, h\n",
    "config.latent_size = 1 # random noise size\n",
    "\"\"\"\n",
    "모델 입력 이미지에 수행할 normalization과 모델 생성 결과 이미지에 수행할 denormalization을 정의합니다.\n",
    "\"\"\"\n",
    "config.augmentation = transforms.Compose([\n",
    "                        transforms.Resize((config.img_shape[1], config.img_shape[2])), #resize\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.5], std=[0.5]) #normalization\n",
    "                      ])\n",
    "config.denormalize = lambda x: x*0.5+0.5 #denormalization 위의 단계 reverse , 원래의 이미지를 보기 위해 수행함\n",
    "\n",
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #cuda에 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b950712e",
   "metadata": {
    "id": "b950712e"
   },
   "outputs": [],
   "source": [
    "#지정된 경로의 데이터셋 가져오기 및 저장\n",
    "if not os.path.isdir(config.data_path):\n",
    "    os.makedirs(config.data_path)\n",
    "if not os.path.isdir(os.path.join(config.save_path, config.dataset)):\n",
    "    os.makedirs(os.path.join(config.save_path, config.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29018cfd",
   "metadata": {
    "id": "29018cfd",
    "outputId": "38d86d46-3dc3-4e0f-ad44-7efd5d9a3da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc2c6a",
   "metadata": {
    "id": "3fbc2c6a"
   },
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12be5c3b",
   "metadata": {
    "id": "12be5c3b",
    "outputId": "233c623f-3f2e-4b90-df50-14f3ea03ab33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 8717\n",
      "    Root location: data/resource/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST와 CIFAR-10은 torchvision 라이브러리에서 제공하여 아래와 같이 사용할 수 있습니다.\n",
    "\"\"\"\n",
    "if config.dataset == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(config.data_path,\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=config.augmentation\n",
    "                                  ) \n",
    "elif config.dataset == 'CIFAR10': \n",
    "    train_dataset = datasets.CIFAR10(config.data_path,\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=config.augmentation\n",
    "                                     )\n",
    "elif config.dataset == 'emoji': \n",
    "    train_dataset = datasets.ImageFolder(config.data_path,\n",
    "                                         transform=config.augmentation\n",
    "                                     )\n",
    "\n",
    "print(config.dataset)\n",
    "    \n",
    "\"\"\"\n",
    "training set을 Dataloader에 넣습니다. \n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers = 2)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e22dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8717"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c95863",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634dcb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2abcfdbeb20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(to_pil_image(0.5*img+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff42e1c",
   "metadata": {},
   "source": [
    "## custom data\n",
    "\n",
    "로컬 디렉토리 image 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ffa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688ba052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = ''\n",
    "glob.glob('data/resource/emoji/*')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a936d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2abc9b5d1c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba67a4c",
   "metadata": {
    "id": "6ba67a4c"
   },
   "source": [
    "## GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e81de91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3608b5",
   "metadata": {
    "id": "7f3608b5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 일반적으로, GAN에서는 loss가 Discriminator에서부터 Generator로 흐를 때 생길 수 있는 \n",
    " vanishing gradient 현상을 완화하기 위해 Leaky ReLU를 많이 사용합니다. \n",
    "\"\"\"\n",
    "class Generator(nn.Module): # 입력으로 noise를 받음\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.f1 = nn.Linear(config.latent_size, 100)     \n",
    "        \n",
    "        self.dconv1 = nn.ConvTranspose2d(100, config.ngf*8, 4, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(config.ngf*8)\n",
    "            \n",
    "        self.dconv2 = nn.ConvTranspose2d(config.ngf*8, config.ngf*4, 4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(config.ngf*4)\n",
    "            \n",
    "        self.dconv3 = nn.ConvTranspose2d(config.ngf*4, config.ngf*2, 4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(config.ngf*2)\n",
    "            \n",
    "        self.dconv4 = nn.ConvTranspose2d(config.ngf*2, config.ngf,4,stride=2,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(config.ngf)\n",
    "            \n",
    "        self.dconv5 = nn.ConvTranspose2d(config.ngf, config.img_shape[0], 4,stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.f1(z)\n",
    "        \n",
    "        x = x.reshape(real_img.shape[0], config.latent_size,1,1)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn1(self.dconv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.dconv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.dconv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.dconv4(x)), 0.2)\n",
    "        x = torch.tanh(self.dconv5(x))\n",
    "        #img = img.reshape(img.shape[0], *config.img_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10c2717",
   "metadata": {
    "id": "c10c2717"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(config.img_shape[0],config.ndf,4,stride=2,padding=1,bias=False)\n",
    "        self.conv2 = nn.Conv2d(config.ndf,config.ndf*2,4,stride=2,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(config.ndf*2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(config.ndf*2, config.ndf*4,4,stride=2,padding=1,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(config.ndf*4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(config.ndf*4,config.ndf*8,4,stride=2,padding=1,bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(config.ndf*8)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(config.ndf*8,1,4,stride=1,padding=0,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.conv1(x),0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),0.2)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        return x.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9dc4bf",
   "metadata": {
    "id": "3b9dc4bf"
   },
   "source": [
    "#### Binary Cross Entropy loss between the target and the input probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684f084",
   "metadata": {
    "id": "9684f084"
   },
   "source": [
    "- [torch.nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd9eac2a",
   "metadata": {
    "id": "fd9eac2a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "binary cross entropy loss를 사용하여 adversarial loss를 구현합니다.\n",
    "\"\"\"\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\"\"\"\n",
    "Generator와 Discriminator를 각각 정의하고, 상응하는 optimizer도 함께 정의합니다.\n",
    "\"\"\"\n",
    "generator = Generator(config).to(config.device)\n",
    "discriminator = Discriminator(config).to(config.device)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c9f5670",
   "metadata": {
    "id": "3c9f5670",
    "outputId": "f57c1e89-69d8-46b5-f8b6-cb86430f25b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (f1): Linear(in_features=1, out_features=100, bias=True)\n",
       "  (dconv1): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dconv2): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dconv3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dconv4): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dconv5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4ca9ee",
   "metadata": {
    "id": "cf4ca9ee",
    "outputId": "95a7777c-fc6d-4c55-cf29-7aa6fc6ed9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200b0b3",
   "metadata": {
    "id": "b200b0b3"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee9bd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = torch.randn((100, config.latent_size), device=config.device, dtype=torch.float32)\n",
    "# print(z.shape)\n",
    "# print(z.reshape(100,100,1,1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "482c07e6",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ef5466833de7413dac10e9bce3d17184"
     ]
    },
    "id": "482c07e6",
    "outputId": "61603b83-f92e-4e40-c146-eabf74bb5b65",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a417df3a124d4dbd531ca79c50b756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[150, 1, 1, 1]' is invalid for input of size 15000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 가짜 이미지 생성\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#rint(real_img.shape[0])\u001b[39;00m\n\u001b[0;32m     32\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((real_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], config\u001b[38;5;241m.\u001b[39mlatent_size), device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 34\u001b[0m gen_img \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mDiscriminator가 실제 이미지와 Generator가 생성한 이미지를 잘 구별하는지 loss를 계산합니다.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m이 때, Generator는 현재 계산된 loss로 학습되지 않으므로, \u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mdetach() 함수를 이용하여 생성 이미지를 computation graph에서 분리한 후 Discriminator의 입력으로 넣어줍니다. \u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m## loss 구함\u001b[39;00m\n",
      "File \u001b[1;32mD:\\song\\Program\\anaconda3\\envs\\basis_3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf1(z)\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv1(x)), \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv2(x)), \u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[150, 1, 1, 1]' is invalid for input of size 15000"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "inception = InceptionScore()\n",
    "\"\"\"\n",
    "Generator와 Discriminator를 번갈아 학습합니다.\n",
    "\"\"\"\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "for epoch in tqdm(range(config.n_epoch)):\n",
    "    #if epoch % 10 == 0:\n",
    "        #print(epoch,\"-------\")\n",
    "    for i, (real_img, _) in enumerate(train_loader):\n",
    "        \n",
    "        # real img 불러오기 (batch, 3, 64, 64)\n",
    "        real_img = real_img.to(config.device)\n",
    "        \n",
    "        # discriminator 학습을 위해 real img는 1, fake img는 0으로 학습시킨다.\n",
    "        valid_label = torch.ones((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        fake_label = torch.zeros((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                Train Discriminator                  #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        # 가짜 이미지 생성\n",
    "        #rint(real_img.shape[0])\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "\n",
    "        gen_img = generator(z)\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Discriminator가 실제 이미지와 Generator가 생성한 이미지를 잘 구별하는지 loss를 계산합니다.\n",
    "        이 때, Generator는 현재 계산된 loss로 학습되지 않으므로, \n",
    "        detach() 함수를 이용하여 생성 이미지를 computation graph에서 분리한 후 Discriminator의 입력으로 넣어줍니다. \n",
    "        \"\"\"\n",
    "        \n",
    "        ## loss 구함\n",
    "        real_loss = criterion(discriminator(real_img), valid_label)\n",
    "        fake_loss = criterion(discriminator(gen_img.detach()), fake_label) # detach generator는 가중치 업데이트 안함\n",
    "        d_loss = (real_loss + fake_loss) * 0.5\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Discriminator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward() # 여기서 grediant가 업데이트 안됨\n",
    "        optimizer_d.step()\n",
    "\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                   Train Generator                   #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "        gen_img = generator(z)\n",
    "\n",
    "        \"\"\"\n",
    "        Generator가 Discriminator를 속일 수 있는지 loss를 계산합니다.\n",
    "        \"\"\"\n",
    "        g_loss = criterion(discriminator(gen_img), valid_label) #valid_label로 하여 generator를 학습하게 함\n",
    "        \n",
    " \n",
    "        \n",
    "        \"\"\"\n",
    "        Generator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if (epoch+1) % config.save_interval == 0:\n",
    "                fid = FrechetInceptionDistance(feature=64)\n",
    "                real = torch.tensor(real_img,dtype=torch.uint8).cpu()\n",
    "                fake = torch.tensor(gen_img,dtype=torch.uint8).cpu()\n",
    "        \n",
    "                fid.update(real, real=True)\n",
    "                fid.update(fake, real=False)\n",
    "                \n",
    "                inception.update(fake)\n",
    "        \n",
    "                if i == 0:\n",
    "                    is_s1,is_s2  = inception.compute()\n",
    "                    fid_s = fid.compute()\n",
    "                else:\n",
    "                    is_s1,is_s2 = inception.compute()\n",
    "                    is_s1 += is_s1\n",
    "                    is_s2 += is_s2\n",
    "                    \n",
    "                    fid_s +=fid.compute()\n",
    "                    \n",
    "                    \n",
    "                if (i+1) % config.log_interval == 0: \n",
    "                    print('Epoch [{}/{}] Batch [{}/{}] Discriminator fid: {} is1:{} is2:{}'.format(\n",
    "                    epoch+1,\n",
    "                    config.n_epoch,\n",
    "                    i+1,\n",
    "                    len(train_loader),\n",
    "                    fid_s/(i+1),\n",
    "                    is_s1/(i+1),\n",
    "                    is_s2/(i+1)\n",
    "                    ))\n",
    "  \n",
    "        if (i+1) % config.log_interval == 0:\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            d_loss_list.append(d_loss.item())\n",
    "            print('Epoch [{}/{}] Batch [{}/{}] Discriminator loss: {:.4f} Generator loss: {:.4f}'.format(\n",
    "                epoch+1,\n",
    "                config.n_epoch,\n",
    "                i+1,\n",
    "                len(train_loader),\n",
    "                d_loss.item(),\n",
    "                g_loss.item()\n",
    "            ))\n",
    "                \n",
    "            \n",
    "    if (epoch+1) % config.save_interval == 0:\n",
    "        save_path = os.path.join(config.save_path, config.dataset, 'epoch_[{0:-03d}]_fid[{1}].png'.format(\n",
    "            epoch+1,\n",
    "            fid_s/(i+1)\n",
    "        ))\n",
    "        gen_img = config.denormalize(gen_img)\n",
    "        torchvision.utils.save_image(gen_img.data[:16], save_path, nrow=4, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0658ce",
   "metadata": {
    "id": "0e0658ce",
    "outputId": "f697deea-a400-44f8-8133-0bc0b179596c"
   },
   "outputs": [],
   "source": [
    "plt.title('GAN training loss on {} data'.format(config.dataset))\n",
    "plt.plot(g_loss_list, label='generator loss')\n",
    "plt.plot(d_loss_list, label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('GAN training loss on {} data'.format(config.dataset))\n",
    "plt.plot(g_loss_list[1:], label='generator loss')\n",
    "plt.plot(d_loss_list[1:], label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18473b",
   "metadata": {
    "id": "3b18473b"
   },
   "source": [
    "## Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b0509",
   "metadata": {
    "id": "db1b0509",
    "outputId": "71f5ad53-0e15-4dd8-ac74-29a7126017ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(config.save_path, config.dataset)\n",
    "for image_path in os.listdir(save_path):\n",
    "    if image_path.endswith('.png'):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        image = Image.open(os.path.join(save_path, image_path))\n",
    "        plt.title(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee394e6",
   "metadata": {
    "id": "1ee394e6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "basis_3.8",
   "language": "python",
   "name": "basis_3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e361de83e42006260aed2b055f8f04aac5dcd7e3eec4c7f8d8ccd6db537cc702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
